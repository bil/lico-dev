# __DRIVER_CODE__ imports

import time
from cython.operator cimport dereference as deref
from libc.string cimport strncpy

cdef extern from "<msgpack.h>" nogil:
  ctypedef int (*msgpack_packer_write)(void* data, const char* buf, size_t len)
  ctypedef struct msgpack_sbuffer:
    size_t size
    char* data
    size_t alloc
  ctypedef struct msgpack_packer:
    void* data
    msgpack_packer_write callback
  void msgpack_sbuffer_destroy(msgpack_sbuffer* sbuf)
  void msgpack_sbuffer_clear(msgpack_sbuffer* sbuf)
  int msgpack_sbuffer_write(void* data, const char* buf, size_t len)
  void msgpack_sbuffer_init(msgpack_sbuffer* sbuf)
  int msgpack_pack_array(msgpack_packer* pk, size_t n)
  void msgpack_packer_init(msgpack_packer* pk, void* data, msgpack_packer_write callback)
  int msgpack_pack_uint8(msgpack_packer* pk, uint8_t d)
  int msgpack_pack_uint16(msgpack_packer* pk, uint16_t d)
  int msgpack_pack_uint32(msgpack_packer* pk, uint32_t d)
  int msgpack_pack_uint64(msgpack_packer* pk, uint64_t d)
  int msgpack_pack_int8(msgpack_packer* pk, int8_t d)
  int msgpack_pack_int16(msgpack_packer* pk, int16_t d)
  int msgpack_pack_int32(msgpack_packer* pk, int32_t d)
  int msgpack_pack_int64(msgpack_packer* pk, int64_t d)
  int msgpack_pack_float(msgpack_packer* pk, float d)
  int msgpack_pack_double(msgpack_packer* pk, double d)

cdef extern from "loggingStruct.h" nogil:
  ctypedef struct SQLTableElement:
    char *colName
    char *type
    short number
    char *unit
    void *data
    char *SQLtype

  ctypedef struct SQLTable:
    char* tableName
    short numCol
    SQLTableElement *columns

cdef extern from "<sqlite3.h>" nogil:
  ctypedef struct sqlite3:
    pass
  ctypedef struct sqlite3_stmt:
    pass
  ctypedef struct sqlite3_blob:
    pass
  int sqlite3_close(sqlite3 *)
  int sqlite3_exec(
    sqlite3*,                                  
    const char *sql,                           
    int (*callback)(void*,int,char**,char**),  
    void *,                                    
    char **errmsg                              
  )
  void sqlite3_free(void*)
  enum: SQLITE_OK
  ctypedef void (*destructor)(void*)
  destructor SQLITE_STATIC
  enum: SQLITE_OPEN_READWRITE
  int sqlite3_open_v2(
    const char *filename,   
    sqlite3 **ppDb,         
    int flags,              
    const char *zVfs        
  )

cdef extern from "sqlHelpers.h" nogil:
  void sql_bind_int(sqlite3_stmt *stmt, int index, const char* dtype, const void* value)
  void sql_bind_int64(sqlite3_stmt *stmt, int index, const void* value)
  void sql_bind_double(sqlite3_stmt *stmt, int index,  const char* dtype, const void* value)
  void sql_bind_text(sqlite3_stmt *stmt, int index, const void* value, int numBytes, destructor destruct)
  void sql_bind_blob(sqlite3_stmt *stmt, int index, const void* value, int numBytes, destructor destruct)
  void sql_prepare(sqlite3 *db, const char *zSql, int nByte, sqlite3_stmt **ppStmt, const char **pzTail)
  void sql_step(sqlite3_stmt *stmt)
  void sql_finalize(sqlite3_stmt *stmt)
  void openDatabase(sqlite3 **db, char *startName, int db_index, char* newNameLocation)
  void createTables(sqlite3 *db, SQLTable* databaseArr, int numTables)
  cdef const char *INSERT_STR
  cdef const char *VALUES_STR
  cdef const char *OPEN_BRACE
  cdef const char *QUESTION
  cdef const char *CLOSE_BRACE_SEMI
  cdef const char *CLOSE_BRACE
  cdef const char *COMMA


cdef extern from "stdatomic.h":
  enum memory_order:
    memory_order_relaxed,
    memory_order_consume,
    memory_order_acquire,
    memory_order_release,
    memory_order_acq_rel,
    memory_order_seq_cst
  void atomic_thread_fence(memory_order)

cdef extern from "constants.h":
  cdef const char *DATALOGGER_SIGNALS_TABLE_NAME
  enum: NEW_DB_NUM_TICKS
  enum: SQL_LOGGER_FLUSH


# __DRIVER_CODE__ variables

# TODO convert to #define in a separate .h file
num_tick_table = {{tick_table|length}}
num_custom_tables = {{custom_tables|length}}

# TODO throw in some declarative type hinting (aka volatile)
# TODO or try converting these to structs/arrays
# TODO can also try with unoptimized compile

# jinja-templated vars
in_sig_keys = {{in_sig_keys}}
in_sig_sigs = {{in_signals}}
tick_table_keys = {{tick_table.keys()|list}}
tick_table = {{tick_table}}
custom_table_sigs = {{custom_table_sigs}}
custom_table_names = {{custom_table_names}}
custom_tables = {{custom_tables}}
msgpack_sigs = {{msgpack_sigs}}

cdef bool newDB = False
cdef int ret
cdef int rc
cdef char currDbName[32]

# worker thread vars
cdef char *zErrMsg
cdef void * retVal

# table buffer vars
cdef uint32_t sig_buf_size_bytes
cdef uint32_t tableBufSize
cdef uint32_t bufferOffsetCur
cdef uint32_t bufferOffsetStrt
cdef uint32_t bufferOffsetEnd

# struct to hold one tick of data
ctypedef struct tickTableData:
  uint8_t *sigs[{{tick_table|length}}]
  int64_t tick
  uint32_t sigNumPktsRecvd[{{tick_table|length}}]
{%- if tick_table|length == 0 %}
  uint8_t padding
{%- endif %}

# tick buffers to enable batching
cdef tickTableData *tickTableBufStrt = NULL

# struct to hold each custom table signal
ctypedef struct customTableData:
  uint8_t **sigs
  int64_t *ticks
  uint32_t *sigNumPktsRecvd

# signal buffers to enable batching
cdef customTableData *customTableBufStrts[{{custom_tables|length}}]

# sqlite variables
cdef sqlite3 *db
cdef sqlite3_stmt *contStmt
cdef sqlite3_stmt *signalsStmt
cdef SQLTable databaseArray[{{logger_num_tables}}]
cdef SQLTableElement *tickTableCols = NULL;
cdef SQLTableElement *customTableCols[{{custom_tables|length}}]
initialized = False

# sqlite variables
cdef int eNewHalt
cdef char *tickTableQuery
cdef char *customTableQueries[{{custom_tables|length}}]
cdef uint32_t customTableOnes[{{in_sig_keys|length}}]

# message pack
cdef msgpack_sbuffer msgpackBufs[{{msgpack_sigs|length}}]
cdef msgpack_packer msgpackPkrs[{{msgpack_sigs|length}}]

cdef int db_index = 0
cdef uint32_t ticks_written = 0
cdef int i, j, k

tick_count = 0
flushing = False

cdef int msgpack_pack(dtype, msgpack_packer* pk, void *pVal, offset):
  if dtype == "uint8":
    return msgpack_pack_uint8(pk, (<uint8_t *>pVal)[offset])
  elif dtype == "uint16":
    return msgpack_pack_uint16(pk, (<uint16_t *>pVal)[offset])
  elif dtype == "uint32":
    return msgpack_pack_uint32(pk, (<uint32_t *>pVal)[offset])
  elif dtype == "uint64":
    return msgpack_pack_uint64(pk, (<uint64_t *>pVal)[offset])
  elif dtype == "int8":
    return msgpack_pack_int8(pk, (<int8_t *>pVal)[offset])
  elif dtype == "int16":
    return msgpack_pack_int16(pk, (<int16_t *>pVal)[offset])
  elif dtype == "int32":
    return msgpack_pack_int32(pk, (<int32_t *>pVal)[offset])
  elif dtype == "int64":
    return msgpack_pack_int64(pk, (<int64_t *>pVal)[offset])
  elif dtype == "float":
    return msgpack_pack_float(pk, (<float *>pVal)[offset])
  elif dtype == "double":
    return msgpack_pack_double(pk, (<double *>pVal)[offset])

cdef void msgpack_init(
  msgpack_sbuffer *msgpackBufs, msgpack_packer *msgpackPkrs,
  in_sig_sigs, msgpack_sigs
):
  cdef uint64_t zeroVal = 0
  for sig, args in in_sig_sigs.items():
    if args["log"]["enable"] and args["log"]["type"] == "msgpack":
      m_i = msgpack_sigs.index(sig)
      msgpack_sbuffer_init(&msgpackBufs[m_i])
      msgpack_packer_init(
        &msgpackPkrs[m_i], &msgpackBufs[m_i], msgpack_sbuffer_write
      )

      # enlarge buffer before main execution
      max_bytes = args["max_packets_per_tick"] * args["packet_size"]
      msgpack_pack_array(&msgpackPkrs[m_i], max_bytes)
      for _ in range(max_bytes):
        msgpack_pack(
          args["dtype_msgpack"], &msgpackPkrs[m_i], <void*>&zeroVal, 0
        )

# Malloc memory for dst and perform a strncpy with null-termination
cdef void* py2cStr(char** dst, src):
  n = len(src)
  dst[0] = <char *> malloc(n + 1)
  strncpy(dst[0], <char *>src, n)
  dst[0][n] = b'\0'

cdef void* createQuery(
  char **query, const char* tableName, int numCols
):
  cdef int queryLen = (
    strlen(INSERT_STR) + strlen(tableName) +
    strlen(VALUES_STR) + strlen(OPEN_BRACE) +
    (strlen(QUESTION) + strlen(COMMA)) * numCols - strlen(COMMA) +
    strlen(CLOSE_BRACE_SEMI)
  )
  query[0] = <char *> malloc(queryLen + 1)
  strcpy(query[0], INSERT_STR)
  strcat(query[0], tableName)
  strcat(query[0], VALUES_STR)
  strcat(query[0], OPEN_BRACE)
  for i in range(numCols):
    strcat(query[0], QUESTION)
    if (i != numCols - 1):
      strcat(query[0], COMMA)
  strcat(query[0], CLOSE_BRACE_SEMI)

cdef void* prepareTable(
  char *dbName, SQLTable *dbArr, SQLTableElement **cols, int numCols,
  sigs
):
  cols[0] = <SQLTableElement *> malloc(numCols * sizeof(SQLTableElement))
  # prepare SQL database table
  cdef int num_extra_cols = 0

  py2cStr(
    &cols[0][num_extra_cols].colName,
    "tick_num".encode("ascii")
  )
  cols[0][num_extra_cols].SQLtype = "INTEGER"
  num_extra_cols += 1

  for i, (sig, args) in enumerate(sigs.items()):
    if args["log"]["type"] == "msgpack": # msgpack sigs
      py2cStr(
        &cols[0][i + num_extra_cols].colName,
        f"m_{args['dtype_short']}_{sig}".encode("ascii")
      )
      cols[0][i + num_extra_cols].SQLtype = "BLOB"

    elif args["log"]["type"] == "vector": # vector sigs
      for j in range(args["log"]["num_cols"]):
        if "suffixes" in args["log"]:
          py2cStr(
            &cols[0][i + num_extra_cols].colName,
            f"v_{args['dtype_short']}_{sig}_{args['log']['suffixes'][j]}".encode("ascii")
          )
        else:
          py2cStr(
            &cols[0][i + num_extra_cols].colName,
            f"v_{args['dtype_short']}_{sig}_{j}".encode("ascii")
          )
        if "int" in args["dtype"]:
          cols[0][i + num_extra_cols].SQLtype = "INTEGER"
        else: # float
          cols[0][i + num_extra_cols].SQLtype = "REAL"

        num_extra_cols += 1

      num_extra_cols -= 1 # only count *extra* added cols

    elif args["log"]["type"] == "text": # text sigs
      py2cStr(
        &cols[0][i + num_extra_cols].colName,
        f"t_{args['dtype_short']}_{sig}".encode("ascii")
      )
      cols[0][i + num_extra_cols].SQLtype = "TEXT"

    elif args["log"]["type"] == "scalar": # raw number sigs
      py2cStr(
        &cols[0][i + num_extra_cols].colName,
        f"r_{args['dtype_short']}_{sig}".encode("ascii")
      )
      if "int" in args["dtype"]:
        cols[0][i + num_extra_cols].SQLtype = "INTEGER"
      else: # float
        cols[0][i + num_extra_cols].SQLtype = "REAL"

  dbArr[0].tableName = dbName
  dbArr[0].numCol = numCols
  dbArr[0].columns = cols[0]


# perform copies from signal buffers to logger buffers
cdef void* bufferSignals(
  uint32_t bufferOffsetCur,
  uint32_t tableBufSize,
  tickTableData *tickTableBufStrt,
  customTableData **customTableBufStrts,
):
  global customTableBufStrt, customTableBufEnd, tickTableBufStrt, sig_lens, sigs

  cdef tickTableData *pTickDataCur
  cdef customTableData *pCustomDataCur

  # for each tick buffered, copy each signal from buffer into pTickDataCur
  for i, sig in enumerate(tick_table_keys):
    # s_i = in_sig_keys.index(sig)
    args = tick_table[sig]
    # TODO this logic should have some validation
    # in the case of max_packets_per_tick != 1, must be saved as a blob or be
    # in its own table

    pTickDataCur = tickTableBufStrt + bufferOffsetCur
    memcpy(
      pTickDataCur.sigs[i],
      <void *><long>sigs[sig].__array_interface__["data"][0],
      sig_lens[sig] * args["bytes"]
    )
    if args["max_packets_per_tick"] == 1:
      pTickDataCur.sigNumPktsRecvd[i] = 1
    else:
      pTickDataCur.sigNumPktsRecvd[i] = sigs[sig].shape[0]

    pTickDataCur.tick = tick_num

  atomic_thread_fence(memory_order_seq_cst)

  for i, table_name in enumerate(custom_table_names):
    pCustomDataCur = customTableBufStrts[i] + bufferOffsetCur
    table_sigs = custom_table_sigs[table_name]
    for j, sig in enumerate(table_sigs):
      args = custom_tables[table_name][sig]
      memcpy(
        pCustomDataCur.sigs[j],
        <void *><long>(sigs[sig].__array_interface__["data"][0]),
        sig_lens[sig] * args["bytes"]
      )
      if args["max_packets_per_tick"] == 1:
        pCustomDataCur.sigNumPktsRecvd[j] = 1
      else:
        pCustomDataCur.sigNumPktsRecvd[j] = sigs[sig].shape[0]

    sig_len = sig_lens[table_sigs[0]]
    for sig in table_sigs:
      assert sig_lens[sig] == sig_len
    for j in range(sig_len):
      pCustomDataCur.ticks[j] = tick_num

  atomic_thread_fence(memory_order_seq_cst)


cdef void performFlush(
  sig_keys, sigs,
  char *query, sqlite3_stmt *stmt,
  uint8_t **sigBufs, int64_t *tickPtr, uint32_t *sigNumPktsRecvd, int pkt_num
):
  global msgpack_sigs, msgpackBufs, msgpackPkrs

  if len(sig_keys) == 0:
    return

  sql_prepare(db, query, -1, &stmt, NULL)

  cdef int num_extra_cols = 0 # restart counter for each row loggedc
  cdef uint8_t *pSig

  if tickPtr != NULL:
    sql_bind_int64(stmt, 1, tickPtr)
    num_extra_cols += 1

  for i, sig in enumerate(sig_keys):
    args = sigs[sig]
    pSig = sigBufs[i] + <int>(pkt_num * args["bytes"])
    if args["log"]["type"] == "msgpack": # msgpack signal
      m_i = msgpack_sigs.index(sig)
      msgpack_sbuffer_clear(&msgpackBufs[m_i])
      msgpack_pack_array(
        &msgpackPkrs[m_i], args["packet_size"] * sigNumPktsRecvd[i]
      )
      for j in range (args["packet_size"] * sigNumPktsRecvd[i]):
        msgpack_pack(
          args["dtype_msgpack"],
          &msgpackPkrs[m_i],
          pSig,
          j
        )
      sql_bind_blob(
        stmt,
        i + 1 + num_extra_cols,
        msgpackBufs[m_i].data,
        msgpackBufs[m_i].size,
        SQLITE_STATIC
      )

    elif args['log']['type'] == "vector": # vector signal
      for j in range(args["log"]["num_cols"]):
        if "int" in args["dtype"]:
          if "64" in args["dtype"]:
            sql_bind_int64(
              stmt,
              i + 1 + num_extra_cols,
              &((<int64_t *>pSig)[j])
            )
          else:
            sql_bind_int(
              stmt,
              i + 1 + num_extra_cols,
              args["dtype"].encode("utf-8"),
              &((<uint8_t *>pSig)[j * args["bytes"]])
            )
        else: # float
          sql_bind_double(
            stmt,
            i + 1 + num_extra_cols,
            args["dtype"].encode("utf-8"),
            &((<uint8_t *>pSig)[j * args["bytes"]])
          )
        num_extra_cols += 1
      num_extra_cols -= 1

    elif args["log"]["type"] == "text": # text signal
      sql_bind_text(
        stmt,
        i + 1 + num_extra_cols,
        pSig,
        args["log"]["numBytes"] * args["packet_size"],
        SQLITE_STATIC
      )

    elif args["log"]["type"] == "scalar": # raw number signal
      if "int" in args["dtype"]:
        if "64" in args["dtype"]:
          sql_bind_int64(
            stmt, i + 1 + num_extra_cols, pSig
          )
        else:
          sql_bind_int(
            stmt,
            i + 1 + num_extra_cols,
            args["dtype"].encode("utf-8"),
            pSig
          )
      else: # float
        sql_bind_double(
          stmt,
          i + 1 + num_extra_cols,
          args["dtype"].encode("utf-8"),
          pSig
        )

  sql_step(stmt)
  sql_finalize(stmt)

cdef void flushToDisk(bool newDB=False):
  global zErrMsg, db, newDB, db_index, exitMask, signalsStmt, \
    bufferOffsetStrt, bufferOffsetEnd, tickTableBufStrt, customTableBufStrts, \
    ticks_written, tickTableQuery, customTableQueries, flushing, \
    custom_table_keys, custom_tables, tableBufSize

  flushing = True
  complete = True

  cdef tickTableData *pTickDataStrt
  cdef customTableData *pCustomDataStrt
  # cdef uint8_t *dataPtr

  # create new database if time limit reached
  if (newDB):
    db_index += 1
    openDatabase(
      &db, "{{ out_signal['args']['save_file']}}", db_index, currDbName
    )
    ret = sqlite3_exec(db, "PRAGMA journal_mode = MEMORY", NULL, NULL, &zErrMsg)
    if (ret != SQLITE_OK):
      printf("%s\n", zErrMsg[0])
      sqlite3_free(zErrMsg)
      printf("SQL pragma error\n")
      fflush(stdout)
      return
    createTables(db, databaseArray, {{logger_num_tables}})

  ret = sqlite3_exec(db, "BEGIN TRANSACTION", NULL, NULL, &zErrMsg)
  if (ret != SQLITE_OK):
    printf("%s\n", zErrMsg[0])
    sqlite3_free(zErrMsg)
    printf("SQL begin transaction error: %d\n", ret)
    fflush(stdout)
    return

  while (bufferOffsetStrt != bufferOffsetEnd):
    pTickDataStrt = tickTableBufStrt + bufferOffsetStrt
    # TODO validate:
    # tick-view vector and scalar signals must have max_packets_per_tick == 1
    performFlush(
      tick_table_keys,
      tick_table,
      tickTableQuery,
      signalsStmt,
      pTickDataStrt.sigs,
      &pTickDataStrt.tick,
      pTickDataStrt.sigNumPktsRecvd,
      0
    )

    for i, table_name in enumerate(custom_table_names):
      pCustomDataStrt = customTableBufStrts[i] + bufferOffsetStrt
      sig_pkts = pCustomDataStrt.sigNumPktsRecvd[0]
      for j in range(1, len(custom_tables[table_name])):
        assert sig_pkts == pCustomDataStrt.sigNumPktsRecvd[j]
      for j in range(sig_pkts):
        performFlush(
          custom_table_sigs[table_name],
          custom_tables[table_name],
          customTableQueries[i],
          signalsStmt,
          pCustomDataStrt.sigs,
          pCustomDataStrt.ticks,
          customTableOnes,  # flush one packet at a time
          j
        )

    bufferOffsetStrt += 1
    if (bufferOffsetStrt == tableBufSize):
      bufferOffsetStrt = 0

    ticks_written += 1
    if (
      (ticks_written % NEW_DB_NUM_TICKS == 0) and
      (ticks_written < {{ config["config"]["num_ticks"] }})
    ):
      complete = False
      break

  ret = sqlite3_exec(db, "END TRANSACTION", NULL, NULL, &zErrMsg)
  if (ret != SQLITE_OK):
    printf("%s\n", zErrMsg[0])
    sqlite3_free(zErrMsg)
    printf("SQL end transaction error\n")
    fflush(stdout)
    return

  if not complete:
    flushToDisk(newDB=True)

  flushing = False


# __DRIVER_CODE__ setup

# create numpy signals table sql insert queries
{%- if tick_table|length != 0 %}
createQuery(
  &tickTableQuery,
  DATALOGGER_SIGNALS_TABLE_NAME,
  {{tick_table|length}} + {{tick_view_extra_cols}} + 1
)
{%- endif %}
{%- for table_name in custom_table_names %}
num_cols = 1
  {%- for sig, args in custom_tables[table_name].items() %}
num_cols += {{args["log"]["num_cols"]}}
  {%- endfor %}
createQuery(
  &customTableQueries[{{loop.index0}}],
  <char *>b"{{table_name}}",
  num_cols
)
{%- endfor %}

# assumes flush occurs in less than SQL_LOGGER_FLUSH ticks
tableBufSize = 2 * SQL_LOGGER_FLUSH
bufferOffsetCur = 0
bufferOffsetStrt = 0
bufferOffsetEnd = 0

# tick-view signals buffer
tickTableBufStrt = <tickTableData *>malloc(
  sizeof(tickTableData) * tableBufSize
)
for i, sig in enumerate(tick_table_keys):
  args = tick_table[sig]
  sig_buf_size_bytes = args["buf_size_bytes"]
  tickTableBufStrt[0].sigs[i] = <uint8_t *>malloc(
    sig_buf_size_bytes * tableBufSize
  )
  for j in range(1, tableBufSize):
    tickTableBufStrt[j].sigs[i] = tickTableBufStrt[0].sigs[i] + <uint64_t>(
      j * sig_buf_size_bytes
    )

# per-signal buffers
for i, table_name in enumerate(custom_table_names):
  customTableBufStrts[i] = <customTableData *>malloc(
    sizeof(customTableData) * tableBufSize
  )

  # allocate memory for signal data
  table_sigs = custom_table_sigs[table_name]
  # this is validated to be the same for all table_sigs
  args = custom_tables[table_name][table_sigs[0]]
  tk_buf_size_bytes = (
    args["packet_size"] * args["max_packets_per_tick"] * sizeof(int64_t)
  )

  # allocate buffers for ticks and counters and pointer buffer for signals
  # for each struct
  customTableBufStrts[i][0].sigs = <uint8_t **>malloc(
    sizeof(uint8_t *) * len(table_sigs) * tableBufSize
  )
  customTableBufStrts[i][0].ticks = <int64_t *>malloc(
    tk_buf_size_bytes * tableBufSize
  )
  customTableBufStrts[i][0].sigNumPktsRecvd = <uint32_t *>malloc(
    sizeof(uint32_t) * len(table_sigs) * tableBufSize
  )

  for j in range(1, tableBufSize):
    customTableBufStrts[i][j].sigs = (
      customTableBufStrts[i][0].sigs + (
        <uint64_t>(j * sizeof(uint8_t *) * len(table_sigs))
      )
    )
    customTableBufStrts[i][j].ticks = (
      customTableBufStrts[i][0].ticks + (<int>(j * tk_buf_size_bytes))
    )
    customTableBufStrts[i][j].sigNumPktsRecvd = (
      customTableBufStrts[i][0].sigNumPktsRecvd + (
        <int>(j * sizeof(uint32_t) * len(table_sigs))
      )
    )

  # allocate signal data buffers
  for j, sig in enumerate(table_sigs):
    args = custom_tables[table_name][sig]
    sig_buf_size_bytes = args["buf_size_bytes"]
    customTableBufStrts[i][0].sigs[j] = <uint8_t *>malloc(
      sig_buf_size_bytes * tableBufSize
    )

    for k in range(1, tableBufSize):
      customTableBufStrts[i][k].sigs[j] = (
        customTableBufStrts[i][0].sigs[j] + <int>(k * sig_buf_size_bytes)
      )

for i in range(len(in_sig_keys)):
  customTableOnes[i] = 1

dbArrOffset = 0
if num_tick_table != 0:
  prepareTable(
    DATALOGGER_SIGNALS_TABLE_NAME,
    &databaseArray[0],
    &tickTableCols,
    {{tick_table|length}} + {{tick_view_extra_cols}} + 1,
    tick_table,
  )
  dbArrOffset += 1

{% for table_name in custom_table_names %}
  {%- set outer_loop = loop %}
# one column for ticks
num_cols = 1
  {%- for sig, args in custom_tables[table_name].items() %}
num_cols += {{args["log"]["num_cols"]}}
  {%- endfor %}
prepareTable(
  <char *>b"{{table_name}}",
  &databaseArray[{{outer_loop.index0}} + dbArrOffset],
  &customTableCols[{{outer_loop.index0}}],
  num_cols,
  custom_tables["{{table_name}}"],
)
{%- endfor %}


# initialize message pack memory for matrix signals
msgpack_init(
  msgpackBufs, msgpackPkrs, in_sig_sigs, msgpack_sigs
)

openDatabase(&db, "{{ out_signal['args']['save_file'] }}", db_index, currDbName)
rc = sqlite3_exec(db, "PRAGMA journal_mode = MEMORY;", NULL, NULL, &zErrMsg)
if (rc != SQLITE_OK):
  printf("%s\n", zErrMsg[0])
  fflush(stdout)
  sqlite3_free(zErrMsg)
  die("SQL pragma error\n")

if num_tick_table + num_custom_tables != 0:
  createTables(db, databaseArray, {{logger_num_tables}})

initialized = True

# __DRIVER_CODE__ write

{%- if (tick_table|length + custom_tables|length) == 0 %}
  time.sleep(0.01)
{%- else %}

  # for now we assume signals are synchronous and we get one signal each tick.
  # signals can be put in separate tables if needed for multiple async.
  # sufficient ticks must be buffered for async to work. error handling??
  # parsing happens on async side, so need previous tick signals exposed here
  bufferSignals(
    bufferOffsetCur, tableBufSize, tickTableBufStrt, customTableBufStrts
  )
  bufferOffsetCur += 1
  if (bufferOffsetCur == tableBufSize):
    bufferOffsetCur = 0

  # after SQL_LOGGER_FLUSH ticks, flush buffered data to DB
  # TODO this has to be a different var since numTicks is not accurate for async
  tick_count += 1
  if (tick_count % SQL_LOGGER_FLUSH == 0):
    bufferOffsetEnd = bufferOffsetCur
    flushToDisk()


{%- endif %}


# __DRIVER_CODE__ exit_handler

  global msgpackBufs, tickTableBufStrt, tickTableCols, customTableBufStrts, customTableCols, pNumTicks, initialized, flushing, bufferOffsetCur, bufferOffsetEnd, tick_table_keys, custom_table_names

  cdef int rc
  cdef int i, j

  # perform final flush of buffered data
  if initialized and not flushing:
    bufferOffsetEnd = bufferOffsetCur
    flushToDisk()

  if tickTableBufStrt != NULL:
    for i, sig in enumerate(tick_table_keys):
      free(tickTableBufStrt[0].sigs[i])
    free(tickTableBufStrt)

  for i, table_name in enumerate(custom_table_names):
    if customTableBufStrts[i] != NULL:
      for j in range(len(custom_table_sigs[table_name])):
        free(customTableBufStrts[i][0].sigs[j])
      free(customTableBufStrts[i][0].sigNumPktsRecvd)
      free(customTableBufStrts[i][0].ticks)
      free(customTableBufStrts[i][0].sigs)
      free(customTableBufStrts[i])

  if tickTableCols != NULL:
    for i in range(len(tick_table)):
      free(tickTableCols[i].colName)
    free(tickTableCols)

  for i, table_name in enumerate(custom_table_names):
    if customTableCols[i] != NULL:
      for j in range(len(custom_table_sigs[table_name])):
        free(customTableCols[i][j].colName)
      free(customTableCols[i])

  # destroy msgpack signals
  for i in range(len(msgpack_sigs)):
    msgpack_sbuffer_destroy(&msgpackBufs[i])

  # clean up sqlite db
  rc = sqlite3_close(db)
  if (rc != SQLITE_OK):
    printf("SQL close error\n")
    fflush(stdout)
